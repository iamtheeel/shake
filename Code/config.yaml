expTrackDir: "exp_Track"

debugs:
    debug: True 
    writeValData: True
    saveModelInfo: True
    runModel: True
    testNormCorr: False #This loads up the entire matrix

plts:
    pltDir: "plots"
    animDir: "../animations" #Link on my MAC
    generatePlots: False
    showFilesForAnimation: False #Show instead of save

    #For CWT plottited data
    rgbPlotChList: [6, 5, 4] # For 3 channel plot, 0 to use the chList
    #rgbPlotChList: [7, 6, 5] # For 3 channel plot, 0 to use the chList
    #saveCombined: True
    #saveOverlay: False
    #saveInline: False
    
    #yLim: [1, 3]
    yLim_freqD: [0, 0.2]
    #min: -0.04518992181415571, max: 0.042370559493933264
    yLim_timeD: [-0.02, 0.02]
    #yLim: 0 #Find the limit

    fulLenFreqYLim:  10
    windowedFreqYLim:  1 #0.5

data:
    trainRatio: .8 # 80% train, 20% of the data for validation
    inputData: "../TestData" # The input data
    dataOutDir: "../data_out" # Link on the MAC

    test: "Test_2" #One person fixed pace, straight path, has labels
    #test: "Test_3" #One person, down hall, into and around lab, has labels, 
    #test: "Test_4" #Multiple people variyed pace, no labels

    classes: ['No Step', '001', '002', '003']
    chList: [8, 7, 6, 5, 4, 3, 2, 16, 1] # 
    #chList: [6, 5, 4] # Try for 3 color plot
    #chList: [10, 9, 8] # Try for 3 color plot
    #chList: [7, 6, 5] # Try for 3 color plot
    #chList: [7, 8, 9, 10] # Test 2, Note: looking at 8Z. seems dead
    #chList: [7, 6, 5, 4, 3, 2, 16, 1] # Test 2, Note: Sensor 8Z seems dead
    #chList: [6, 5, 4, 3, 2, 1, 10, 9, 11, 12, 13] # test 3
    #sensorChList: [1, 2, 3, 4, 5, 6, 7, 10, 11, 14, 15, 16, 17, 18, 19, 20 ] # Just the Z chans
    #sensorChList: [[1], [2], [3], [4], [5], [6], [7], [8, 9, 10], [11, 12, 13], [14], [15], [16], [17], [18], [19], [20] ]

    limitRuns: 0 #1 #2 # Limit the number of runs to load: 0 to load all the data
    limitWindowLen: 0 #15 #20 # Limit the number of windows to load: 0 to load all

    ## Depricated
    #stompSens: [5, 6]
    stompSens: [5, 6, 7]
    ## Use stomp from file
    stompThresh: "stompTimes.csv" #1.5 #If 0, then no stomp detection

    dataThresh: 2 # If 0, then no, non-walking detection

    windowLen: 5 #2 #sec
    stepSize: 1 #1 #sec


    dataScalers: ["std"]
    #dataScalers: ["std", "meanNorm"]
    labelScalers: ["std"]
    #dataScalers: ["meanNorm", "minMaxNorm", "std", "none"]
    #labelScalers: ["meanNorm", "minMaxNorm", "std"]
    #scalers: ["meanNorm", "std", "minMaxNorm"]
    dataScale_values: [1]   #
    labelScale_values: [1] 
    dataMax: 1 #0 for calculate
    #scale: 100 # Classification

cwt:
    #F0 is the center frequency (higher values = better frequency resolution)
    #B is the bandwidth parameter (higher values = better time resolution)
    # cmor give diagnal lines, no good
    # PUt this in exp track, and save the plots
    ### wavelets for pywt
    # Family	Prefix	Description
    # Haar	haar	Simplest wavelet (equivalent to db1)
    # Daubechies	db	Compactly supported orthogonal wavelets
    # Coiflets	coif	Wavelets with vanishing moments for smooth functions
    # Symlets	sym	Symmetric versions of Daubechies wavelets
    # Biorthogonal	bior	Used in image compression (e.g., JPEG2000)
    # Reverse Biorthogonal	rbio	Similar to bior, with reversed filters
    # Meyer	meyer	Smooth wavelet with an infinite support
    # Morlet	morl	Continuous wavelet used for time-frequency analysis
    # Mexican Hat	mexh	Second derivative of a Gaussian function
    # Gaussian	gaus	Gaussian wavelets
    # ['bior1.1', 'bior1.3', 'bior1.5', 'bior2.2', 'bior2.4', 'bior2.6', 'bior2.8', 'bior3.1', 'bior3.3', 'bior3.5', 'bior3.7', 'bior3.9', 'bior4.4', 'bior5.5', 'bior6.8', 'cgau1', 'cgau2', 'cgau3', 'cgau4', 'cgau5', 'cgau6', 'cgau7', 'cgau8', 'cmor', 'coif1', 'coif2', 'coif3', 'coif4', 'coif5', 'coif6', 'coif7', 'coif8', 'coif9', 'coif10', 'coif11', 'coif12', 'coif13', 'coif14', 'coif15', 'coif16', 'coif17', 'db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8', 'db9', 'db10', 'db11', 'db12', 'db13', 'db14', 'db15', 'db16', 'db17', 'db18', 'db19', 'db20', 'db21', 'db22', 'db23', 'db24', 'db25', 'db26', 'db27', 'db28', 'db29', 'db30', 'db31', 'db32', 'db33', 'db34', 'db35', 'db36', 'db37', 'db38', 'dmey', 'fbsp', 'gaus1', 'gaus2', 'gaus3', 'gaus4', 'gaus5', 'gaus6', 'gaus7', 'gaus8', 'haar', 'mexh', 'morl', 'rbio1.1', 'rbio1.3', 'rbio1.5', 'rbio2.2', 'rbio2.4', 'rbio2.6', 'rbio2.8', 'rbio3.1', 'rbio3.3', 'rbio3.5', 'rbio3.7', 'rbio3.9', 'rbio4.4', 'rbio5.5', 'rbio6.8', 'shan', 'sym2', 'sym3', 'sym4', 'sym5', 'sym6', 'sym7', 'sym8', 'sym9', 'sym10', 'sym11', 'sym12', 'sym13', 'sym14', 'sym15', 'sym16', 'sym17', 'sym18', 'sym19', 'sym20']

    doCWT: True #Roll this in to "wavelet"
    saveCWT: True

    logScaleFreq: False
    numScales: 256 # Seems like a good number to work with
    fMin: 1 #0.5 0 = 0... so don't use 0 makes for div by 0
    fMax: 100 #5 #0 = use the nyquist
    wavelet: ["cmor"] #"None" for no CWT
    #wavelet: ["morl", "cmorl"] # f0 = 0.8125, bw = 6.0
    #wavelet: ["cmor", "fstep" ]
    #waveLet_center_freq:  [2.14] #fstep center freq best4 for fStep
    #waveLet_center_freq:  [2.14] #fstep center freq best4 for fStep 0.5 to 10hz
    #waveLet_center_freq:  [0.8125] #morl center freq
    waveLet_center_freq:  [10] # best for cmorl
    waveLet_bandwidth: [0.8]  # best vor cmorl
    #Bandwidth nopt used for fstep
    #waveLet_bandwidth: [6.0] 
    # Norms to each ch/data if normTo_max is 0
    ### Min and max for cwtplots
    #normTo_max: 0.016 #Max for linear scale, but only a few outliers are this high
    #normTo_max: 0.010 #0 for none #Fstep
    normTo_max: 0
    #normTo_max: 0 #10 #11.15 #Max for log scale
    normTo_min: 0 #1.81
    #normTo_max: 8.708692765932984 #Max for log scale
    #normTo_max: 8.92159461776423 #Max for log scale
    logScale: False

trainer:
    # 16 too big on Mac, or at least with firefox running (note: retry after dataloader fix)
    #batchSize: [8, 16, 32, 64, 128] #8 #For mobilenet timeD, redo 64 and 128 longer
    #batchSize: [1, 2, 4 ] #8 #For mobilenet timeD 4 failed with mem error
    #batchSize: [64, 128] 

    #batchSize: [8, 16, 32] #8 #For mobilenet timeD
    batchSize: [1] #Should be power of 2

    #criterion_regresh: "Huber" # for regresion
    #criterion_regresh: "MAE" # for regresion
    loss_regresh: ["MSE"] # for regresion
    #loss_regresh: ["MSE", "MAE", "Huber"] # for regresion
    loss_class: ["CrossEntropyLoss"] #For classification

    optimizer: ["Adam"]
    #optimizer: ["Adam", "SGD"]
    # Best for Mobilnet Time Not folded
    learning_rate: [0.00005] #[0.00075] 
    weight_decay:  [0.002, 0.001, 0.0005]  #[0.005]
    #learning_rate: [0.002] #Trying scheduler
    #weight_decay:  [0.01]
    #Best for TimeD folded LeNet (all batch sizes). Could probably use fine tuning
    #learning_rate: [0.00001] 
    #weight_decay:  [0.00001]
    #learning_rate: [0.0001] # Try with optimiser
    #weight_decay:  [0.0001]
    # Time D folded Mobilnet
    #learning_rate:  [0.00005, 0.000075, 0.0001, 0.00025, 0.0005, 0.00075] # Not converging
    #weight_decay: [0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075] # # Not converging
    #learning_rate: [0.000001, 0.000005, 0.00001, 0.00005] #Still not converging
    #weight_decay:  [0.00001, 0.0001, 0.001]
    #learning_rate: [0.0002, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009] #Still not converging
    #weight_decay:  [0.00005, 0.0001, 0.0005, 0.001, 0.002]
    #Worked! lr: 0.0005, wd: 0.0001, 0.001
    #Worked! lr: 0.0006, wd: 0.0005, 0.002
    #Worked! lr: 0.0007, wd: 0.00005, 0.0001
    #Worked! lr: 0.0008, wd: 0.0001
    #learning_rate: [0.0006] 
    #weight_decay:  [0.002]
    # Learing rate schetuler use 2x to 3x the lr
    LR_sch: None #None to not use
    #LR_sch: 'CosineAnnealingWarmRestarts' #None to not use
    T_0: 10       # Restart every N epochs (hop over local minima)
    T-mult: 2     # A factor by which :math:`T_{i}` increases after a restart. Default: 1
    eta_min: 1e-7 # Minimum learning rate. Default: 0
    #LR_sch: 'ReduceLROnPlateau' #None to not use

    gradiant_noise: [0] #[1e-3]

    epochs: 100 #Set to upper limit and validate every... (see below)
    epochValiStart: 0
    validEveryNEpocchs: 1
    seed: 4601

model: 
    regression: True #Or Clasification
    #name: ["leNetV5"]
    #name: ["MobileNet_v2_folded"] # Don't spend time here, we are doing fantastic in the time domain
    name: ["MobileNet_v2"] #
    #name: ["leNetV5_unFolded"] #To send the timeDomain as a bunch of points
    #name: ["MobileNet_v2", "leNetV5"] # The models we have
    #
    timeDImgHeight: 64 #240
    dropOut: [0] #0.5 Not implemented on leNet

    multilayerPerceptron:
        hidden_neurons: 100

    leNetV5:
        hidden_neurons: 100
